{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Download NLTK's stopwords for data cleaning\n",
    "stop = stopwords.words('english')\n",
    "stop.extend('wa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"PATH/TO/CHANNEL/FOLDERS/\"\n",
    "\n",
    "# Make a list of all channel folders\n",
    "dirs = glob(path+\"*/\", recursive=True)\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "j = len(dirs) # Used for tracking progress when building corpus below\n",
    "print('Folders found:', j)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG CELL FOR MANUAL SELECTION\n",
    "users = ['Enter users of interest here to manually select them for a quick corpus']\n",
    "files = []\n",
    "c = []\n",
    "corpus = []\n",
    "\n",
    "for user in users:\n",
    "  files.extend(glob(path + user + \"/*.csv\", recursive=True))\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    col = ''\n",
    "    if f.endswith('.pkl'):\n",
    "      c = pd.read_pickle(f)\n",
    "      col = 'body'\n",
    "    elif f.endswith('.xlsx'):\n",
    "      c = pd.read_excel(f)\n",
    "    elif f.endswith('.csv'):\n",
    "      if f.endswith('.txt.csv'):\n",
    "        try:\n",
    "          c = pd.read_csv(f, usecols=range(3), lineterminator='\\n', quoting=3)\n",
    "        except:\n",
    "          c = pd.read_csv(f, usecols=range(3), lineterminator='\\n', quoting=3, encoding=\"ISO-8859-1\")\n",
    "        c.columns = c.columns.str.replace('\\r','')\n",
    "      else:\n",
    "        c = pd.read_csv(f, index_col='Unnamed: 0')\n",
    "      col = 'comment'\n",
    "    else:\n",
    "      continue\n",
    "    c[col] = c[col].str.replace('\\r','')\n",
    "    corpus.extend(c[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean files and build corpus\n",
    "i = 0       # Current iteration\n",
    "vocab = dict() # For data visualization\n",
    "corpus = [] # For model training\n",
    "skip = True # Used with debug line\n",
    "\n",
    "for folder in dirs:\n",
    "  print('================== Reading files for channel:',folder.split('\\\\')[-2])\n",
    "  print(f' *** This is folder number {i} of {j} ({ int(i/j*100) }%)')\n",
    "  files = glob(folder+\"/*\", recursive=True)\n",
    "  c = pd.DataFrame()\n",
    "\n",
    "  # Debug line to catch problematic files\n",
    "  # if [Condition]:\n",
    "  #   skip = False\n",
    "  # if skip is True:\n",
    "  #   continue\n",
    "\n",
    "  # Handle various formats for Twitch chat datasets\n",
    "  for cur_file in files:\n",
    "    col = ''\n",
    "    if cur_file.endswith('.pkl'):\n",
    "      c = pd.read_pickle(cur_file)\n",
    "      col = 'body'\n",
    "    elif cur_file.endswith('.xlsx'):\n",
    "      c = pd.read_excel(cur_file)\n",
    "    elif cur_file.endswith('.csv'):\n",
    "      if cur_file.endswith('.txt.csv'):\n",
    "        try:\n",
    "          c = pd.read_csv(cur_file, usecols=range(3), lineterminator='\\n', quoting=3)\n",
    "        except:\n",
    "          c = pd.read_csv(cur_file, usecols=range(3), lineterminator='\\n', quoting=3, encoding=\"ISO-8859-1\")\n",
    "        c.columns = c.columns.str.replace('\\r','')\n",
    "      else:\n",
    "        c = pd.read_csv(cur_file, index_col='Unnamed: 0')\n",
    "      col = 'comment'\n",
    "    else:\n",
    "      continue\n",
    "\n",
    "    for msg in c[col]:\n",
    "      # ADD TO CORPUS\n",
    "      corpus.append(msg)\n",
    "\n",
    "      # ADD TO VOCAB FOR DATA VISUALIZATION (commented out: done separately below)\n",
    "      # try:\n",
    "      #   words = str(msg).split()\n",
    "      #   for word in words:\n",
    "      #       if(word != x):\n",
    "      #         word = x\n",
    "      #       if(word not in stop and word is not np.nan):\n",
    "      #         if(word in vocab):\n",
    "      #           vocab[word] += 1\n",
    "      #         else:\n",
    "      #           vocab[word] = 1\n",
    "      # except:\n",
    "      #   print('Error:',words)\n",
    "\n",
    "  # print('Vocab size:',len(vocab))\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "with open('vocab.txt','wb') as f:\n",
    "   pickle.dump(vocab, f)\n",
    "\n",
    "with open('corpus.txt', 'wb') as f:\n",
    "   pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data (if necessary)\n",
    "with open('vocab.txt','rb') as f:\n",
    "   vocab = pickle.load(f)\n",
    "\n",
    "with open('corpus.txt','rb') as f:\n",
    "   corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [x for x in corpus if x is not np.nan]\n",
    "corpus_clean = []\n",
    "\n",
    "# Remove stop words\n",
    "for msg in corpus:\n",
    "    corpus_clean.append(' '.join([word for word in str(msg).split(' ') if word not in stop]))\n",
    "\n",
    "print(corpus_clean[:10]) # Print a sample message to ensure corpus is correctly formatted and accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    \"\"\"Basic cleaning of texts.\"\"\"\n",
    "    \n",
    "    # Remove HTML markup\n",
    "    text=re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "    # Remove non-ascii\n",
    "    text=re.sub(\"(\\\\W)\",\" \",text)\n",
    "    \n",
    "    # Remove whitespace\n",
    "    text=text.strip()\n",
    "    return text\n",
    "\n",
    "# Minor cleaning before stemming for visualization\n",
    "corpus_clean=[scrub_words(w) for w in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_stemmed_words=[]\n",
    "\n",
    "# Stem cleaned corpus\n",
    "for msg in corpus_clean:\n",
    "    cleaned_stemmed_words.append(' '.join([stemmer.stem(word=word) for word in msg.split(' ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare raw vs cleaned vs stemmed\n",
    "stemdf= pd.DataFrame({'raw_word': corpus,'cleaned_word':corpus_clean,'stemmed_word': cleaned_stemmed_words})\n",
    "stemdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurrences of each word in the corpus\n",
    "vocab = {}\n",
    "\n",
    "for msg in cleaned_stemmed_words:\n",
    "    words = str(msg).split(' ')\n",
    "    for word in words:\n",
    "        if(word not in stop and word is not np.nan):\n",
    "            if(word in vocab):\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stemmed\n",
    "with open('vocab_stem.txt', 'wb') as f:\n",
    "   pickle.dump(vocab, f)\n",
    "   \n",
    "with open('corp_clean_stem.txt', 'wb') as f:\n",
    "   pickle.dump(cleaned_stemmed_words, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stemmed (if necessary)\n",
    "with open('vocab_stem.txt','rb') as f:\n",
    "   vocab = pickle.load(f)\n",
    "\n",
    "with open('corp_clean_stem.txt','rb') as f:\n",
    "   corpus_clean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save some memory (only if you didn't load the data in the cell above)\n",
    "corpus_clean = cleaned_stemmed_words\n",
    "del cleaned_stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort vocab by decreasing frequency\n",
    "sorted_dict = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_dict[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the keys and values from the sorted dictionary\n",
    "keys = [k for k, v in sorted_dict[:150]]\n",
    "values = [v for k, v in sorted_dict[:150]]\n",
    "\n",
    "# Use matplotlib to create a bar chart\n",
    "plt.figure(figsize=(25,10))\n",
    "plt.title('Most Frequently Used Words')\n",
    "plt.margins(x=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Vocabulary')\n",
    "plt.ylabel('Frequency')\n",
    "plt.bar(keys, values, color='Purple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word cloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    stopwords=stop,\n",
    "    collocations=False,\n",
    "    scale=6)\n",
    "\n",
    "def show_wordcloud(data):\n",
    "    return wordcloud.generate_from_frequencies(data)\n",
    "\n",
    "plt.figure(figsize=(24, 12))\n",
    "\n",
    "wordcloud = show_wordcloud(vocab)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_ngram(corpus, n=None, count=10):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) \n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of bigrams\n",
    "top_n_bigrams=get_top_ngram(corpus_clean,n=2,count=30)\n",
    "x,y=map(list,zip(*top_n_bigrams))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of trigrams\n",
    "top_tri_grams=get_top_ngram(corpus_clean,n=3,count=30)\n",
    "x,y=map(list,zip(*top_tri_grams))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lens = [] # Length of messages, in characters\n",
    "corpus_words = [] # Length of messages, in words\n",
    "\n",
    "for msg in corpus:\n",
    "    corpus_lens.append(len(msg))\n",
    "    corpus_words.append(len(msg.split(' ')))\n",
    "\n",
    "corpus_lens = Counter(corpus_lens)\n",
    "corpus_words = Counter(corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = { word: occurrences for word, occurrences in corpus_lens.items() if word <= 300 } # Cut off length for graph = 300 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot length of chat messages, in characters\n",
    "plt.figure(figsize=(25,10))\n",
    "plt.title('Chat Message Lengths')\n",
    "plt.margins(x=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Message Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.bar(lens.keys(), lens.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = { word: occurrences for word, occurrences in corpus_words.items() if word <= 25 } # Cut off length for graph = 25 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot length of chat messages, in words\n",
    "plt.figure(figsize=(25,10))\n",
    "plt.title('Chat Word Counts')\n",
    "plt.margins(x=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "bar = plt.bar(words.keys(), words.values())\n",
    "plt.bar_label(bar)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3deea60006abea4783390f213adebd6836966db637e5dfe493c1abe5c82560f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
