{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim.models.word2vec as w2v\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitch emotes\n",
    "with open('emote_dict', 'rb') as f:\n",
    "    emotes = pickle.load(f)\n",
    "    emotes.replace(['NaN', 'nan'], np.nan, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some example models you could train and compare using the dataset provided\n",
    "model = w2v.Word2Vec.load(r'models\\DoubleLift\\model')\n",
    "dbl_embeddings = model.wv\n",
    "del model\n",
    "\n",
    "model = w2v.Word2Vec.load(r'models\\LCS\\model')\n",
    "lcs_embeddings = model.wv\n",
    "del model\n",
    "\n",
    "model = w2v.Word2Vec.load(r'models\\LEC\\model')\n",
    "lec_embeddings = model.wv\n",
    "del model\n",
    "\n",
    "model = w2v.Word2Vec.load(r'models\\Nightblue3\\model')\n",
    "nb3_embeddings = model.wv\n",
    "del model\n",
    "\n",
    "model = w2v.Word2Vec.load(r'models\\Sneaky\\model')\n",
    "sneaky_embeddings = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_between_emotes(emote_list, embeddings):\n",
    "    # Create a dictionary to store the emote vectors\n",
    "    emote_vectors = {}\n",
    "    for emote in emote_list:\n",
    "        if emote in embeddings.index_to_key:\n",
    "            emote_vectors[emote] = embeddings[emote]\n",
    "\n",
    "    # Compute pairwise similarity scores between emotes\n",
    "    similarity_scores = {}\n",
    "    for i, emote1 in enumerate(emote_vectors):\n",
    "        for j, emote2 in enumerate(emote_vectors):\n",
    "            if i < j: # Only do each pair once, and skip equal comparisons\n",
    "                similarity_scores[(emote1, emote2)] = cosine_similarity(\n",
    "                    emote_vectors[emote1].reshape(1, -1),       # Swap rows/columns (1 col x many rows -> 1 row x many cols)\n",
    "                    emote_vectors[emote2].reshape(1, -1))[0][0] # Returns a single value nested in 2D array\n",
    "\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model1: w2v.KeyedVectors, model2: w2v.KeyedVectors):\n",
    "    # Create a set of emotes that are present in both model embeddings\n",
    "    emotes_in_both = set(model1.index_to_key).intersection(set(model2.index_to_key))\n",
    "    emotes_in_both = [x for x in emotes['GLOBAL_TWITCH'].str.lower() if x in emotes_in_both]\n",
    "\n",
    "    # Compute similarity scores between global Twitch emotes in the two embeddings\n",
    "    model1_similarities = similarity_between_emotes(emotes_in_both, model1)\n",
    "    model2_similarities = similarity_between_emotes(emotes_in_both, model2)\n",
    "\n",
    "    # Compute aggregate statistics on the similarity scores\n",
    "    model1_scores = np.array(list(model1_similarities.values()))\n",
    "    model2_scores = np.array(list(model2_similarities.values()))\n",
    "    scores_diff = model1_scores - model2_scores\n",
    "\n",
    "    mean_diff = np.mean(scores_diff)\n",
    "    std_dev_diff = np.std(scores_diff)\n",
    "    corr_coef, _ = pearsonr(model1_scores, model2_scores)\n",
    "\n",
    "    # Print the aggregate statistics\n",
    "    print(\"Aggregate statistics of similarity scores between global Twitch emotes in the two embeddings:\")\n",
    "    print(f\"Mean difference: {mean_diff:.3f}\")\n",
    "    print(f\"Standard deviation of difference: {std_dev_diff:.3f}\")\n",
    "    print(f\"Correlation coefficient: {corr_coef:.3f}\")\n",
    "\n",
    "    return mean_diff, std_dev_diff, corr_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NB3 model : Sneaky model')\n",
    "compare_models(nb3_embeddings, sneaky_embeddings)\n",
    "print()\n",
    "print('NB3 model : Dbl model')\n",
    "compare_models(nb3_embeddings, dbl_embeddings)\n",
    "print()\n",
    "print('Sneaky model : Dbl model')\n",
    "compare_models(sneaky_embeddings, dbl_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LCS model : Sneaky model')\n",
    "compare_models(lcs_embeddings, sneaky_embeddings)\n",
    "print()\n",
    "print('LCS model : Dbl model')\n",
    "compare_models(lcs_embeddings, dbl_embeddings)\n",
    "print()\n",
    "print('LCS model : NB3 model')\n",
    "compare_models(lcs_embeddings, nb3_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LEC model : Sneaky model')\n",
    "compare_models(lec_embeddings, sneaky_embeddings)\n",
    "print()\n",
    "print('LEC model : Dbl model')\n",
    "compare_models(lec_embeddings, dbl_embeddings)\n",
    "print()\n",
    "print('LEC model : NB3 model')\n",
    "compare_models(lec_embeddings, nb3_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model to analyze\n",
    "scan_embeddings = twitch_500_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Create a list of word vectors\n",
    "X = np.array([scan_embeddings[word] for word in scan_embeddings.index_to_key])\n",
    "\n",
    "# Loop through a range of cluster sizes\n",
    "for n_clusters in range(2,10):\n",
    "    # Fit the KMeans model to the word vectors\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    # Calculate the silhouette score\n",
    "    score = silhouette_score(X, kmeans.labels_)\n",
    "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved output\n",
    "\n",
    "# Silhouette Scores from KMeans:\n",
    "#\n",
    "# For n_clusters = 2 The average silhouette_score is : 0.29998946\n",
    "# For n_clusters = 3 The average silhouette_score is : 0.34632137\n",
    "# For n_clusters = 4 The average silhouette_score is : 0.0039549596\n",
    "# For n_clusters = 5 The average silhouette_score is : 0.009397833\n",
    "# For n_clusters = 6 The average silhouette_score is : 0.011708869\n",
    "# For n_clusters = 7 The average silhouette_score is : 0.022252025\n",
    "# For n_clusters = 8 The average silhouette_score is : 0.023897517\n",
    "# For n_clusters = 9 The average silhouette_score is : 0.024429243\n",
    "# For n_clusters = 10 The average silhouette_score is : 0.022800706\n",
    "# For n_clusters = 20 The average silhouette_score is : -0.04120358\n",
    "# For n_clusters = 30 The average silhouette_score is : -0.027650462\n",
    "# For n_clusters = 40 The average silhouette_score is : -0.030612223\n",
    "# For n_clusters = 50 The average silhouette_score is : -0.06241339\n",
    "# For n_clusters = 60 The average silhouette_score is : -0.059461888\n",
    "# For n_clusters = 70 The average silhouette_score is : -0.048446592\n",
    "# For n_clusters = 80 The average silhouette_score is : -0.070959754\n",
    "# For n_clusters = 90 The average silhouette_score is : -0.06987077\n",
    "# For n_clusters = 100 The average silhouette_score is : -0.07816283\n",
    "# For n_clusters = 200 The average silhouette_score is : -0.08500795\n",
    "# For n_clusters = 300 The average silhouette_score is : -0.094385035\n",
    "# For n_clusters = 400 The average silhouette_score is : -0.08832128\n",
    "# For n_clusters = 500 The average silhouette_score is : -0.095110245\n",
    "# For n_clusters = 1000 The average silhouette_score is : -0.09550029\n",
    "# For n_clusters = 2000 The average silhouette_score is : -0.09596107\n",
    "# For n_clusters = 5000 The average silhouette_score is : -0.119661234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Apply t-SNE to reduce the dimensionality of the word embeddings\n",
    "tsne = TSNE(n_components=3, random_state=0)\n",
    "embeddings_2d = tsne.fit_transform(X)\n",
    "\n",
    "# Plot the t-SNE output\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=20).fit(scan_embeddings.vectors)\n",
    "\n",
    "labels = dbscan.labels_\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise)\n",
    "\n",
    "print(f\"Silhouette Coefficient: {silhouette_score(scan_embeddings.vectors, labels):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "\n",
    "clustering = AgglomerativeClustering(distance_threshold=0, \n",
    "                                     n_clusters=None).fit(scan_embeddings.vectors)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(clustering, truncate_mode=\"level\", p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()\n",
    "\n",
    "range_n_clusters = range(2, 30)\n",
    "for n_clusters in range_n_clusters:\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters).fit(scan_embeddings.vectors)\n",
    "\n",
    "    labels = clustering.labels_\n",
    "    silhouette_avg = silhouette_score(scan_embeddings.vectors, labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "n_components = np.arange(2, 30)\n",
    "models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(scan_embeddings.vectors)\n",
    "          for n in n_components]\n",
    "\n",
    "plt.plot(n_components, [m.bic(scan_embeddings.vectors) for m in models], \n",
    "         '-mD', markevery=[6], label='BIC')\n",
    "plt.plot(n_components, [m.aic(scan_embeddings.vectors) for m in models], \n",
    "         '-yD', markevery=[6], label='AIC')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components')\n",
    "\n",
    "clustering = GaussianMixture(6, covariance_type='full', \n",
    "                             random_state=0).fit(scan_embeddings.vectors)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
